{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6c6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4215c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the variables from file : dictionary and tdf/corpus\n",
    "with open(r\"C:\\Users\\kanis\\Major-Project\\Tokens\\tkn01.pkl\",\"rb\") as fp:\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "dictionary = data[0]\n",
    "termdocfreq = data[1]\n",
    "finallemma = data[2]\n",
    "feedtodict = data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6384808b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.048*\"be\" + 0.034*\"learn\" + 0.029*\"word\" + 0.023*\"language\" + 0.015*\"more\" '\n",
      "  '+ 0.015*\"thing\" + 0.015*\"phrase\" + 0.014*\"memorize\" + 0.014*\"t\" + '\n",
      "  '0.013*\"help\"'),\n",
      " (1,\n",
      "  '0.038*\"be\" + 0.030*\"language\" + 0.028*\"phrase\" + 0.026*\"learn\" + '\n",
      "  '0.023*\"read\" + 0.021*\"try\" + 0.020*\"thing\" + 0.018*\"t\" + 0.017*\"important\" '\n",
      "  '+ 0.016*\"Mastering\"'),\n",
      " (2,\n",
      "  '0.044*\"be\" + 0.039*\"learn\" + 0.032*\"language\" + 0.028*\"word\" + 0.014*\"t\" + '\n",
      "  '0.013*\"memorize\" + 0.013*\"help\" + 0.012*\"more\" + 0.011*\"time\" + '\n",
      "  '0.010*\"study\"'),\n",
      " (3,\n",
      "  '0.046*\"be\" + 0.045*\"learn\" + 0.027*\"language\" + 0.019*\"t\" + 0.019*\"phrase\" '\n",
      "  '+ 0.017*\"thing\" + 0.017*\"word\" + 0.015*\"try\" + 0.015*\"most\" + 0.015*\"read\"'),\n",
      " (4,\n",
      "  '0.045*\"be\" + 0.041*\"learn\" + 0.035*\"language\" + 0.029*\"word\" + 0.016*\"t\" + '\n",
      "  '0.012*\"help\" + 0.012*\"new\" + 0.011*\"memorize\" + 0.011*\"re\" + 0.011*\"s\"'),\n",
      " (5,\n",
      "  '0.057*\"be\" + 0.043*\"try\" + 0.039*\"easy\" + 0.039*\"thing\" + 0.038*\"watch\" + '\n",
      "  '0.037*\"show\" + 0.037*\"movie\" + 0.034*\"language\" + 0.022*\"Possibly\" + '\n",
      "  '0.021*\"do\"'),\n",
      " (6,\n",
      "  '0.049*\"be\" + 0.043*\"learn\" + 0.038*\"word\" + 0.024*\"language\" + 0.014*\"t\" + '\n",
      "  '0.014*\"help\" + 0.013*\"important\" + 0.012*\"memorize\" + 0.012*\"n\" + '\n",
      "  '0.012*\"phrase\"'),\n",
      " (7,\n",
      "  '0.044*\"learn\" + 0.036*\"be\" + 0.031*\"language\" + 0.023*\"word\" + '\n",
      "  '0.020*\"phrase\" + 0.014*\"t\" + 0.014*\"thing\" + 0.012*\"help\" + 0.012*\"easy\" + '\n",
      "  '0.011*\"more\"'),\n",
      " (8,\n",
      "  '0.041*\"be\" + 0.039*\"learn\" + 0.030*\"language\" + 0.027*\"word\" + 0.015*\"t\" + '\n",
      "  '0.014*\"s\" + 0.013*\"n\" + 0.013*\"thing\" + 0.012*\"important\" + 0.012*\"more\"'),\n",
      " (9,\n",
      "  '0.052*\"be\" + 0.047*\"learn\" + 0.028*\"word\" + 0.024*\"language\" + '\n",
      "  '0.020*\"memorize\" + 0.015*\"t\" + 0.015*\"help\" + 0.014*\"thing\" + 0.013*\"day\" + '\n",
      "  '0.013*\"phrase\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=termdocfreq,\n",
    "                                       id2word=dictionary,\n",
    "                                       num_topics=num_topics,random_state=100, chunksize=100, passes=10, per_word_topics=True)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[termdocfreq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c2449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "file10 = open(r\"C:\\Users\\kanis\\Major-Project\\Intermediate\\itm01.txt\",\"r+\")\n",
    "\n",
    "Lines = file10.readlines()\n",
    "#print(Lines)\n",
    "x = \"\"\n",
    "for line in Lines:\n",
    "    x = x + line\n",
    "#print(x)\n",
    "tksen = tokenize.sent_tokenize(x)\n",
    "print(len(tksen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad8a00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0::['be', 'learn', 'word', 'language', 'more']\n",
      "1::['be', 'language', 'phrase', 'learn', 'read']\n",
      "2::['be', 'learn', 'language', 'word', 't']\n",
      "3::['be', 'learn', 'language', 't', 'phrase']\n",
      "4::['be', 'learn', 'language', 'word', 't']\n",
      "5::['be', 'try', 'easy', 'thing', 'watch']\n",
      "6::['be', 'learn', 'word', 'language', 't']\n",
      "7::['learn', 'be', 'language', 'word', 'phrase']\n",
      "8::['be', 'learn', 'language', 'word', 't']\n",
      "9::['be', 'learn', 'word', 'language', 'memorize']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lexical sentence weighting \n",
    "\n",
    "x=lda_model.show_topics(num_topics=num_topics, num_words=5,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "#Below Code Prints Topics and Words\n",
    "for topic,words in topics_words:\n",
    "    print(str(topic)+ \"::\"+ str(words))\n",
    "print()\n",
    "\n",
    "dict = {}\n",
    "\n",
    "\n",
    "for topic,words in topics_words:\n",
    "    for word in words :\n",
    "        dict[word] = 0\n",
    "#print(dict)\n",
    "\n",
    "s_w = {}\n",
    "for sentence in tksen:\n",
    "    count = 0\n",
    "    for word in sentence.split() :\n",
    "        \n",
    "        if word in dict:\n",
    "            count = count+1\n",
    "    s_w[sentence] = count/len(sentence)\n",
    "keys = list(s_w.keys())\n",
    "values = list(s_w.values())\n",
    "sorted_value_index =np.argsort(values)\n",
    "sorted_dict = {keys[i]: values[i] for i in sorted_value_index}\n",
    "\n",
    "\n",
    "\n",
    "#print(s_w)\n",
    "avgvalue = 0\n",
    "for ele in s_w:\n",
    "    avgvalue += s_w[ele]\n",
    "    \n",
    "avgvalue = (avgvalue)/len(s_w)\n",
    "summarylex = []\n",
    "for ele in s_w:\n",
    "    if s_w[ele] > avgvalue :\n",
    "        summarylex.append(ele)\n",
    "#print(len(summarylex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4595484e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Possibly the easiest thing you can do is watch television shows or movies in the language you are trying to learn.', \"To make things easier, try to watch shows or movies whose plots you are already familiar withâ€”like kids' cartoons or dubbed versions of English movieâ€”knowing the context will help you to decipher the meanings of words and phrases.\", 'As your language level progresses, you can move on to more advanced reading material like newspapers and magazines.', 'When you read out loud, rather than silently, you can work on both your overall reading comprehension and pronunciation skills.', 'Itâ€™s more important to master basic phrases youâ€™ll use frequently than it is to start off by learning the alphabet or the perfect sentence structure.', \"Therefore, it's important that when you learn a word, you learn the pronunciation simultaneously.\", 'This is pretty much backwardsâ€”if you want to learn a language quickly, you should learn how to converse first.']\n"
     ]
    }
   ],
   "source": [
    "#sliding window sentence weighting \n",
    "window_size = 9\n",
    "sw_sw = {}\n",
    "for sentence in tksen :\n",
    "    weight = -1e9\n",
    "    words  =  sentence.split()\n",
    "    i = 0 \n",
    "    j = 9\n",
    "    if j>len(words):\n",
    "        count = 0\n",
    "        for word in words :\n",
    "            if word in dict :\n",
    "                count += 1\n",
    "        weight = max(weight,count/window_size)\n",
    "        break\n",
    "    while j<len(words):\n",
    "        count = 0\n",
    "        for k in range(j-i):\n",
    "            if words[k] in dict :\n",
    "                count += 1\n",
    "        weight = max(weight,count/window_size)\n",
    "        i = i+1\n",
    "        j = j+1\n",
    "    sw_sw[sentence] = weight\n",
    "#print(sw_sw)\n",
    "avgvalue = 0\n",
    "for ele in sw_sw:\n",
    "    avgvalue += sw_sw[ele]\n",
    "    \n",
    "avgvalue = (avgvalue)/len(sw_sw)\n",
    "slidingwindowsumm = []\n",
    "for ele in sw_sw:\n",
    "    if sw_sw[ele] > avgvalue :\n",
    "        slidingwindowsumm.append(ele)\n",
    "print(slidingwindowsumm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d08924e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence:  0.42157048179208756\n",
      "\n",
      "Perplexity:  -5.0261946621355165\n"
     ]
    }
   ],
   "source": [
    "#coherence and score \n",
    "from gensim.models import CoherenceModel\n",
    "# instantiate topic coherence model\n",
    "import numpy as np\n",
    "newcorpus = []\n",
    "for lis in finallemma :\n",
    "    for w in lis :\n",
    "        if len(w)>2:\n",
    "            newcorpus.append(w)\n",
    "\n",
    "cm = CoherenceModel(model=lda_model, texts=feedtodict, dictionary=dictionary, coherence='c_v')\n",
    "with np.errstate(invalid='ignore'):\n",
    "    coherence_lda = cm.get_coherence()\n",
    "    print('Coherence: ', coherence_lda)                \n",
    "# getting topic coherence score \n",
    "\n",
    "\n",
    "#getting the perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(termdocfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "896b576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle \n",
    "#import pyLDAvis\n",
    "#import os \n",
    "#import pyLDAvis.gensim\n",
    "\n",
    "\n",
    "#num_topics = 10\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "#vis = pyLDAvis.gensim.prepare(lda_model, termdocfreq, dictionary,sort_topics = 'false')\n",
    "#vis\n",
    "#LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
    "\n",
    "#LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, termdocfreq, dictionary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2342ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a6b7c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.549967</td>\n",
       "      <td>learn, be, language, word, phrase, t, thing, h...</td>\n",
       "      <td>Possibly the easiest thing you can do is watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.699977</td>\n",
       "      <td>be, learn, language, t, phrase, thing, word, t...</td>\n",
       "      <td>Try to avoid subtitles, so you donâ€™t develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.774979</td>\n",
       "      <td>be, try, easy, thing, watch, show, movie, lang...</td>\n",
       "      <td>To make things easier, try to watch shows or m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.819981</td>\n",
       "      <td>be, learn, language, word, t, help, new, memor...</td>\n",
       "      <td>As your language level progresses, you can mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.849981</td>\n",
       "      <td>learn, be, language, word, phrase, t, thing, h...</td>\n",
       "      <td>When you read out loud, rather than silently, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.871413</td>\n",
       "      <td>be, learn, language, word, t, help, new, memor...</td>\n",
       "      <td>Rather than trying to translate each phrase, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.887487</td>\n",
       "      <td>be, learn, word, language, t, help, important,...</td>\n",
       "      <td>Mastering everyday conversation will let you h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.899990</td>\n",
       "      <td>be, learn, word, language, more, thing, phrase...</td>\n",
       "      <td>Itâ€™s more important to master basic phrases ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909991</td>\n",
       "      <td>be, learn, word, language, more, thing, phrase...</td>\n",
       "      <td>Mastering basic vocabulary is probably one of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.918173</td>\n",
       "      <td>be, learn, word, language, memorize, t, help, ...</td>\n",
       "      <td>Even if you can't understand whole sentences, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.924991</td>\n",
       "      <td>be, learn, language, word, t, s, n, thing, imp...</td>\n",
       "      <td>There's no point in memorizing hundreds of wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.930761</td>\n",
       "      <td>be, learn, language, t, phrase, thing, word, t...</td>\n",
       "      <td>Therefore, it's important that when you learn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.935704</td>\n",
       "      <td>be, language, phrase, learn, read, try, thing,...</td>\n",
       "      <td>The reason why most people can't remember most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>0.939991</td>\n",
       "      <td>learn, be, language, word, phrase, t, thing, h...</td>\n",
       "      <td>This is pretty much backwardsâ€”if you want to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.943741</td>\n",
       "      <td>be, learn, language, t, phrase, thing, word, t...</td>\n",
       "      <td>The specifics of grammar will come later.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>0.947051</td>\n",
       "      <td>be, try, easy, thing, watch, show, movie, lang...</td>\n",
       "      <td>Start off with clear goals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.952624</td>\n",
       "      <td>be, learn, language, word, t, help, new, memor...</td>\n",
       "      <td>Try learning to count to ten since 1-10 is usu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>0.954993</td>\n",
       "      <td>be, learn, word, language, t, help, important,...</td>\n",
       "      <td>Each day learn a new set of ten numbers, keep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.957137</td>\n",
       "      <td>be, language, phrase, learn, read, try, thing,...</td>\n",
       "      <td>If you are up for a challenge, memorize all th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0.959085</td>\n",
       "      <td>be, learn, word, language, memorize, t, help, ...</td>\n",
       "      <td>In turn, better reading and pronunciation can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>0.960864</td>\n",
       "      <td>be, learn, word, language, t, help, important,...</td>\n",
       "      <td>Plus, it is better for you to be sounding the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>0.962495</td>\n",
       "      <td>be, learn, language, word, t, help, new, memor...</td>\n",
       "      <td>Too often, people spend all of their time stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>0.963995</td>\n",
       "      <td>be, learn, language, word, t, s, n, thing, imp...</td>\n",
       "      <td>Speaking with a real, live person will help yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>0.965380</td>\n",
       "      <td>learn, be, language, word, phrase, t, thing, h...</td>\n",
       "      <td>Carrying a dictionary will allow you to find t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.966663</td>\n",
       "      <td>be, learn, word, language, more, thing, phrase...</td>\n",
       "      <td>This is especially important when you are havi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>0.967853</td>\n",
       "      <td>be, learn, word, language, t, help, important,...</td>\n",
       "      <td>In addition, looking up the word and using it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>0.968962</td>\n",
       "      <td>be, learn, language, word, t, s, n, thing, imp...</td>\n",
       "      <td>Most language learning apps offer both a free ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>0.969996</td>\n",
       "      <td>be, try, easy, thing, watch, show, movie, lang...</td>\n",
       "      <td>Weâ€™ll breakdown each app below:\\nPeople ofte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>0.970964</td>\n",
       "      <td>be, learn, language, word, t, s, n, thing, imp...</td>\n",
       "      <td>But when they say five years, they probably me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0.971872</td>\n",
       "      <td>be, learn, language, word, t, memorize, help, ...</td>\n",
       "      <td>If you want to learn a new language ''quickly'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.972724</td>\n",
       "      <td>be, learn, language, t, phrase, thing, word, t...</td>\n",
       "      <td>When you're learning a new language youâ€™ll g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>0.972724</td>\n",
       "      <td>be, learn, word, language, memorize, t, help, ...</td>\n",
       "      <td>You might find yourself in situations where yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.973526</td>\n",
       "      <td>be, learn, language, word, t, memorize, help, ...</td>\n",
       "      <td>Thatâ€™s all a normal part of the language-lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>0.974283</td>\n",
       "      <td>be, learn, language, t, phrase, thing, word, t...</td>\n",
       "      <td>If you can visit and spend some time in a coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>0.974997</td>\n",
       "      <td>be, learn, language, word, t, memorize, help, ...</td>\n",
       "      <td>Learning a language within the context of cult...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Document_No  Dominant_Topic  Topic_Perc_Contrib   \n",
       "0             0               7            0.549967  \\\n",
       "1             1               3            0.699977   \n",
       "2             2               5            0.774979   \n",
       "3             3               4            0.819981   \n",
       "4             4               7            0.849981   \n",
       "5             5               4            0.871413   \n",
       "6             6               6            0.887487   \n",
       "7             7               0            0.899990   \n",
       "8             8               0            0.909991   \n",
       "9             9               9            0.918173   \n",
       "10           10               8            0.924991   \n",
       "11           11               3            0.930761   \n",
       "12           12               1            0.935704   \n",
       "13           13               7            0.939991   \n",
       "14           14               3            0.943741   \n",
       "15           15               5            0.947051   \n",
       "16           16               4            0.952624   \n",
       "17           17               6            0.954993   \n",
       "18           18               1            0.957137   \n",
       "19           19               9            0.959085   \n",
       "20           20               6            0.960864   \n",
       "21           21               4            0.962495   \n",
       "22           22               8            0.963995   \n",
       "23           23               7            0.965380   \n",
       "24           24               0            0.966663   \n",
       "25           25               6            0.967853   \n",
       "26           26               8            0.968962   \n",
       "27           27               5            0.969996   \n",
       "28           28               8            0.970964   \n",
       "29           29               2            0.971872   \n",
       "30           30               3            0.972724   \n",
       "31           31               9            0.972724   \n",
       "32           32               2            0.973526   \n",
       "33           33               3            0.974283   \n",
       "34           34               2            0.974997   \n",
       "\n",
       "                                             Keywords   \n",
       "0   learn, be, language, word, phrase, t, thing, h...  \\\n",
       "1   be, learn, language, t, phrase, thing, word, t...   \n",
       "2   be, try, easy, thing, watch, show, movie, lang...   \n",
       "3   be, learn, language, word, t, help, new, memor...   \n",
       "4   learn, be, language, word, phrase, t, thing, h...   \n",
       "5   be, learn, language, word, t, help, new, memor...   \n",
       "6   be, learn, word, language, t, help, important,...   \n",
       "7   be, learn, word, language, more, thing, phrase...   \n",
       "8   be, learn, word, language, more, thing, phrase...   \n",
       "9   be, learn, word, language, memorize, t, help, ...   \n",
       "10  be, learn, language, word, t, s, n, thing, imp...   \n",
       "11  be, learn, language, t, phrase, thing, word, t...   \n",
       "12  be, language, phrase, learn, read, try, thing,...   \n",
       "13  learn, be, language, word, phrase, t, thing, h...   \n",
       "14  be, learn, language, t, phrase, thing, word, t...   \n",
       "15  be, try, easy, thing, watch, show, movie, lang...   \n",
       "16  be, learn, language, word, t, help, new, memor...   \n",
       "17  be, learn, word, language, t, help, important,...   \n",
       "18  be, language, phrase, learn, read, try, thing,...   \n",
       "19  be, learn, word, language, memorize, t, help, ...   \n",
       "20  be, learn, word, language, t, help, important,...   \n",
       "21  be, learn, language, word, t, help, new, memor...   \n",
       "22  be, learn, language, word, t, s, n, thing, imp...   \n",
       "23  learn, be, language, word, phrase, t, thing, h...   \n",
       "24  be, learn, word, language, more, thing, phrase...   \n",
       "25  be, learn, word, language, t, help, important,...   \n",
       "26  be, learn, language, word, t, s, n, thing, imp...   \n",
       "27  be, try, easy, thing, watch, show, movie, lang...   \n",
       "28  be, learn, language, word, t, s, n, thing, imp...   \n",
       "29  be, learn, language, word, t, memorize, help, ...   \n",
       "30  be, learn, language, t, phrase, thing, word, t...   \n",
       "31  be, learn, word, language, memorize, t, help, ...   \n",
       "32  be, learn, language, word, t, memorize, help, ...   \n",
       "33  be, learn, language, t, phrase, thing, word, t...   \n",
       "34  be, learn, language, word, t, memorize, help, ...   \n",
       "\n",
       "                                                 Text  \n",
       "0   Possibly the easiest thing you can do is watch...  \n",
       "1   Try to avoid subtitles, so you donâ€™t develop...  \n",
       "2   To make things easier, try to watch shows or m...  \n",
       "3   As your language level progresses, you can mov...  \n",
       "4   When you read out loud, rather than silently, ...  \n",
       "5   Rather than trying to translate each phrase, f...  \n",
       "6   Mastering everyday conversation will let you h...  \n",
       "7   Itâ€™s more important to master basic phrases ...  \n",
       "8   Mastering basic vocabulary is probably one of ...  \n",
       "9   Even if you can't understand whole sentences, ...  \n",
       "10  There's no point in memorizing hundreds of wor...  \n",
       "11  Therefore, it's important that when you learn ...  \n",
       "12  The reason why most people can't remember most...  \n",
       "13  This is pretty much backwardsâ€”if you want to...  \n",
       "14          The specifics of grammar will come later.  \n",
       "15                        Start off with clear goals.  \n",
       "16  Try learning to count to ten since 1-10 is usu...  \n",
       "17  Each day learn a new set of ten numbers, keep ...  \n",
       "18  If you are up for a challenge, memorize all th...  \n",
       "19  In turn, better reading and pronunciation can ...  \n",
       "20  Plus, it is better for you to be sounding the ...  \n",
       "21  Too often, people spend all of their time stud...  \n",
       "22  Speaking with a real, live person will help yo...  \n",
       "23  Carrying a dictionary will allow you to find t...  \n",
       "24  This is especially important when you are havi...  \n",
       "25  In addition, looking up the word and using it ...  \n",
       "26  Most language learning apps offer both a free ...  \n",
       "27  Weâ€™ll breakdown each app below:\\nPeople ofte...  \n",
       "28  But when they say five years, they probably me...  \n",
       "29  If you want to learn a new language ''quickly'...  \n",
       "30  When you're learning a new language youâ€™ll g...  \n",
       "31  You might find yourself in situations where yo...  \n",
       "32  Thatâ€™s all a normal part of the language-lea...  \n",
       "33  If you can visit and spend some time in a coun...  \n",
       "34  Learning a language within the context of cult...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the dominant topics \n",
    "import random\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=termdocfreq, texts=tksen):\n",
    "    # Init output\n",
    "    \n",
    "    dp = []\n",
    "    pc = []\n",
    "    tk = []\n",
    "    \n",
    "    \n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        #print(row_list)\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list \n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                x = random.randint(0, 9)\n",
    "                topic_num=(topic_num+x)%10\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                #print(topic_keywords + \" \"+str(topic_num)+\" \"+str(prop_topic))\n",
    "                #sent_topics_df = pd.concat([sent_topics_df, pd.Series([int(topic_num), round(prop_topic,4), topic_keywords])], ignore_index=True)\n",
    "                #df = pd.DataFrame({'Dominant_Topic': topic_num,'Perc_Contribution':prop_topic,'Topic_Keywords':topic_keywords})\n",
    "                #sent_topics_df = pd.concat([sent_topics_df, df], ignore_index=True)\n",
    "                dp.append(topic_num)\n",
    "                pc.append(prop_topic)\n",
    "                tk.append(topic_keywords)\n",
    "            else:\n",
    "                break\n",
    "   \n",
    "    sent_topics_df = pd.DataFrame({'Dominant_Topic' : dp, 'Perc_contribution':pc, 'topic_keywords':tk })\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "    \n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=termdocfreq, texts = tksen)\n",
    "df_len=len(df_topic_sents_keywords.index)\n",
    "df_topic_sents_keywords=df_topic_sents_keywords.drop(df_topic_sents_keywords.index[len(tksen):])\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_len=len(df_topic_sents_keywords.index)\n",
    "print(df_len)\n",
    "df_dominant_topic.head(df_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a63fb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.966663</td>\n",
       "      <td>be, learn, word, language, more, thing, phrase...</td>\n",
       "      <td>This is especially important when you are havi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.957137</td>\n",
       "      <td>be, language, phrase, learn, read, try, thing,...</td>\n",
       "      <td>If you are up for a challenge, memorize all th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.974997</td>\n",
       "      <td>be, learn, language, word, t, memorize, help, ...</td>\n",
       "      <td>Learning a language within the context of cult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.974283</td>\n",
       "      <td>be, learn, language, t, phrase, thing, word, t...</td>\n",
       "      <td>If you can visit and spend some time in a coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.962495</td>\n",
       "      <td>be, learn, language, word, t, help, new, memor...</td>\n",
       "      <td>Too often, people spend all of their time stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.969996</td>\n",
       "      <td>be, try, easy, thing, watch, show, movie, lang...</td>\n",
       "      <td>Weâ€™ll breakdown each app below:\\nPeople ofte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.967853</td>\n",
       "      <td>be, learn, word, language, t, help, important,...</td>\n",
       "      <td>In addition, looking up the word and using it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.965380</td>\n",
       "      <td>learn, be, language, word, phrase, t, thing, h...</td>\n",
       "      <td>Carrying a dictionary will allow you to find t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.970964</td>\n",
       "      <td>be, learn, language, word, t, s, n, thing, imp...</td>\n",
       "      <td>But when they say five years, they probably me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.972724</td>\n",
       "      <td>be, learn, word, language, memorize, t, help, ...</td>\n",
       "      <td>You might find yourself in situations where yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib   \n",
       "0          0            0.966663  \\\n",
       "1          1            0.957137   \n",
       "2          2            0.974997   \n",
       "3          3            0.974283   \n",
       "4          4            0.962495   \n",
       "5          5            0.969996   \n",
       "6          6            0.967853   \n",
       "7          7            0.965380   \n",
       "8          8            0.970964   \n",
       "9          9            0.972724   \n",
       "\n",
       "                                            Keywords   \n",
       "0  be, learn, word, language, more, thing, phrase...  \\\n",
       "1  be, language, phrase, learn, read, try, thing,...   \n",
       "2  be, learn, language, word, t, memorize, help, ...   \n",
       "3  be, learn, language, t, phrase, thing, word, t...   \n",
       "4  be, learn, language, word, t, help, new, memor...   \n",
       "5  be, try, easy, thing, watch, show, movie, lang...   \n",
       "6  be, learn, word, language, t, help, important,...   \n",
       "7  learn, be, language, word, phrase, t, thing, h...   \n",
       "8  be, learn, language, word, t, s, n, thing, imp...   \n",
       "9  be, learn, word, language, memorize, t, help, ...   \n",
       "\n",
       "                                                Text  \n",
       "0  This is especially important when you are havi...  \n",
       "1  If you are up for a challenge, memorize all th...  \n",
       "2  Learning a language within the context of cult...  \n",
       "3  If you can visit and spend some time in a coun...  \n",
       "4  Too often, people spend all of their time stud...  \n",
       "5  Weâ€™ll breakdown each app below:\\nPeople ofte...  \n",
       "6  In addition, looking up the word and using it ...  \n",
       "7  Carrying a dictionary will allow you to find t...  \n",
       "8  But when they say five years, they probably me...  \n",
       "9  You might find yourself in situations where yo...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(len(tksen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9fd90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet.to_csv(r\"C:\\Users\\kanis\\Major-Project\\PreClusteringData\\mrdfet.csv\")\n",
    "#saving the most representative document for each topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c2b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv(r\"C:\\Users\\kanis\\Major-Project\\PreClusteringData\\dt.csv\")\n",
    "#saving the topic wise information for each sentence or document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11e32411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "\n",
    "#making the list of clusters with their sentences classfied accordingly \n",
    "listofclusters = [None] * num_topics\n",
    "for i in range(num_topics):\n",
    "    listofclusters[i] = []\n",
    "\n",
    "with open(r'C:\\Users\\kanis\\Major-Project\\PreClusteringData\\dt.csv', 'r',encoding=\"utf8\") as csvfile :\n",
    "    datareader = csv.reader(csvfile)\n",
    "    for row in datareader:\n",
    "        if row[0] == '':\n",
    "            continue\n",
    "        listofclusters[int(row[2])].append(row[5]) # saving the sentence(row5) in its dominant topic(row2) cluster\n",
    "     \n",
    "    \n",
    "#loop for reprsentation\n",
    "i = 0\n",
    "for item in listofclusters :\n",
    "    #print(\"topic cluster no. \"+str(i))\n",
    "    #for sentence in item:\n",
    "        #print(sentence)\n",
    "    #print()\n",
    "    i = i + 1\n",
    "    \n",
    "#print(listofclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f8b2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d78b43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db4775f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "listofsentence_vectors = []\n",
    "sentence_vectors = []\n",
    "for sentences in listofclusters:\n",
    "    for i in sentences:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        sentence_vectors.append(v)\n",
    "    listofsentence_vectors.append(sentence_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c190a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c385e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "k = 0\n",
    "\n",
    "listofsimmatrix = []\n",
    "for sentences in listofclusters:\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "    #We will use Cosine Similarity to compute the similarity between a pair of sentences.\n",
    "\n",
    "\n",
    "#And initialize the matrix with cosine similarity scores.\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(listofsentence_vectors[k][i].reshape(1,100), listofsentence_vectors[k][j].reshape(1,100))[0,0]\n",
    "    k=k+1\n",
    "    listofsimmatrix.append(sim_mat)\n",
    "#print(len(listofsimmatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe7674d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "listofsubsum = [None] * num_topics\n",
    "for i in range(num_topics):\n",
    "    listofsubsum[i] = []\n",
    "\n",
    "k = 0\n",
    "for sim_mat in listofsimmatrix:\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(listofclusters[k])), reverse=True)\n",
    "    for i in range(int(len(ranked_sentences)/3)):\n",
    "        listofsubsum[k].append(ranked_sentences[i][1])\n",
    "    k=k+1\n",
    "#print(listofsubsum)\n",
    "#print()\n",
    "#print(listofclusters)\n",
    "\n",
    "\n",
    "#generating the final list\n",
    "finallist = []\n",
    "for lis in listofsubsum :\n",
    "    for sent in lis :\n",
    "        if len(sent)>0:\n",
    "            finallist.append(sent)\n",
    "#print(finallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b355dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating dictionary to arrange summaryin chronological order\n",
    "dictofsent = {}\n",
    "k = 0\n",
    "for sent in tksen :\n",
    "    dictofsent[sent] = k\n",
    "    k=k+1\n",
    "summarytosave=sorted(finallist, key=dictofsent.get)\n",
    "#print(summarytosave)\n",
    "\n",
    "sample = \"\"\n",
    "for sent in summarytosave:\n",
    "    sample += sent \n",
    "    sample += ' '\n",
    "\n",
    "listsave = []\n",
    "listsave.append(sample)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9103284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lexrank \n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "446da9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustersummary = [None] * num_topics\n",
    "for i in range(num_topics):\n",
    "    clustersummary[i] = []\n",
    "   \n",
    "k =0\n",
    "for cluster in listofclusters:\n",
    "    sentences = \"\"\n",
    "    for sent in cluster :\n",
    "        sentences += sent\n",
    "        sentences += ' '\n",
    "    p = PlaintextParser.from_string(sentences, Tokenizer(\"english\"))\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summ = summarizer(p.document, int(len(cluster)/3))\n",
    "    for s in summ:\n",
    "        clustersummary[k].append(s)\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78ba4163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you read out loud, rather than silently, you can work on both your overall reading comprehension and pronunciation skills. Rather than trying to translate each phrase, force yourself to think in the language you're reading. Mastering basic vocabulary is probably one of the most important things you can do when learning a new language. Therefore, it's important that when you learn a word, you learn the pronunciation simultaneously. Start off with clear goals. Each day learn a new set of ten numbers, keep going each day until you are satisfied with how high you can count. In turn, better reading and pronunciation can help you memorize words with greater ease. Speaking with a real, live person will help you to feel much more motivated about learning the language than staring at a book or computer screen. Thatâ€™s all a normal part of the language-learning process. \n",
      "\n",
      "Itâ€™s more important to master basic phrases youâ€™ll use frequently than it is to start off by learning the alphabet or the perfect sentence structure. If you want to learn a new language ''quickly'' (that is, in the space of a few weeks or months), you're going to have to commit to studying the language for a couple of hours ''per day''. Try to avoid subtitles, so you donâ€™t develop a reliance on them. As your language level progresses, you can move on to more advanced reading material like newspapers and magazines. To make things easier, try to watch shows or movies whose plots you are already familiar withâ€”like kids' cartoons or dubbed versions of English movieâ€”knowing the context will help you to decipher the meanings of words and phrases. Mastering everyday conversation will let you hit the ground running. Possibly the easiest thing you can do is watch television shows or movies in the language you are trying to learn. There's no point in memorizing hundreds of words and phrases if you pronounce them so oddly that they can't be understood. You might find yourself in situations where you donâ€™t quite know the right word, or you donâ€™t understand 100% of what the other personâ€™s saying. \n"
     ]
    }
   ],
   "source": [
    "finallexsumm = \"\"\n",
    "for subsum in clustersummary:\n",
    "    if len(subsum)==0:\n",
    "        continue\n",
    "    for sent in subsum:\n",
    "        if len(str(sent))>0:\n",
    "            finallexsumm += str(sent)\n",
    "            finallexsumm += ' '\n",
    "#print(finallexsumm)\n",
    "\n",
    "\n",
    "listsave.append(finallexsumm)\n",
    "listsave.append(summarylex)\n",
    "listsave.append(slidingwindowsumm)\n",
    "with open(r\"C:\\Users\\kanis\\Major-Project\\Generated Summary\\gen_sum01.pkl\",\"wb\") as fp:\n",
    "    pickle.dump(listsave,fp)\n",
    "with open(r\"C:\\Users\\kanis\\Major-Project\\Generated Summary\\gen_sum01.pkl\",\"rb\") as fp:\n",
    "    data = pickle.load(fp)\n",
    "    print(data[0])\n",
    "    print()\n",
    "    print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4665fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283eae3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
